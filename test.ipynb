{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "# from my_secrets import OPENAI_API_KEY\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 695, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 1454, which is longer than the specified 500\n",
      "Created a chunk of size 580, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 914, which is longer than the specified 500\n",
      "Created a chunk of size 701, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n",
      "Created a chunk of size 703, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 1114, which is longer than the specified 500\n",
      "Created a chunk of size 757, which is longer than the specified 500\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m----> 6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(texts, embeddings)\n\u001b[1;32m      9\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:350\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    339\u001b[0m client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    348\u001b[0m }\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 350\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params)\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    352\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params\n\u001b[1;32m    354\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('./source_data')\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## Grading rubric for the outcome statement: 3 marks if All outcomes meet criteria 2 marks if Most outcomes meet these criteria: 1 mark if Few outcomes meet these criteria: 0 marks if No outcomes meet these criteria:', metadata={'source': 'data/Rubric_outcomes.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. Clearly named. The name is specific, unique and unambiguous. 2. Accurate. The name does not include a persona, benefit, outcome or JTBD 3. Current state. Represents a scenario that exists currently without your product. 4. Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'data/Rubric_use_cases.txt'}),\n",
       " Document(page_content='Grading Rubric for Problem Space:\\n\\n2 pts: Full Marks for 2 fully-completed problem space candidates were provided. 1 pts: Partial Credit: Only 1 Problem Space candidate was fully-completed. 0 pts: Unacceptable: None of the 2 problem space candidates were fully-completed.', metadata={'source': 'data/Rubricassignment1.txt'}),\n",
       " Document(page_content='## Grading rubric for the Use cases: 3 marks if All use cases satisfy these criteria: 2 marks if Most use cases satisfy these criteria: 1 marks if Few use cases satisfy these criteria: 0 No use cases satisfy these criteria:', metadata={'source': 'data/Rubric_use_cases.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retriever.get_relevant_documents(\n",
    "    \"rubrics?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_for_job_to_be_done_definition\",\n",
    "    \"searches for definition and rubrics for job to be done statement\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='search for job to be done', description='searches for definition and rubrics for job to be done statement', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=<bound method BaseRetriever.get_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17cca0400>)>, coroutine=<bound method BaseRetriever.aget_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17cca0400>)>)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_for_job_to_be_done_definition` with `{'query': 'making a friend'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='for students \"jobs to be done\" include “pursuing a master\\'s degree”, “find housing”, “commuting”, “dining”, “maintaining living space”, “securing employment”, and “traveling”. \"Getting an education\" is one of the central primary jobs. Other primary tasks might include \"attending classes”, “finding part-time work”, “securing internships”, and “dedicating time to study”.\\n\\nUnderstanding all these different jobs could lead you to creating many different products which all businesses eventually need.', metadata={'source': 'data/JTBD.txt'}), Document(page_content='here are a few avenues to explore when gathering data and insights:', metadata={'source': 'data/problem_magnitude.txt'}), Document(page_content='A Fertile Land of Online shopping could have an actor Shopper and merchant\\n\\nThe actor can be as general as \"Teacher,\" encompassing all teachers worldwide, or more specific, such as \"College Teachers,\" narrowing it down to educators at the college level globally. For further precision, you can specify an actor like \"College Teachers at 300 South Craig St, Room 265,\" targeting teachers in a particular room.', metadata={'source': 'data/actor.txt'}), Document(page_content=\"In some instances, it might be challenging to find a precise number. Take, for example, a scenario like the one faced by a student in the class who was dealing with fitness enthusiasts. This can be a complex group to quantify since individuals don't typically wear a label proclaiming their fitness enthusiasm. Thus, determining the opportunity size may require creative strategies, but at its core, it's simply about estimating the number of people who match the characteristics of your actor.\", metadata={'source': 'data/problem_magnitude.txt'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe job to be done statement \"making a friend\" is quite broad and lacks specificity. Jobs to be done statements are typically more specific and tied to a particular task or goal. For example, in a student context, jobs to be done might include \"pursuing a master's degree\", \"finding housing\", or \"securing employment\". \n",
      "\n",
      "To grade this statement, we would need to establish clear rubrics or criteria. These might include specificity, relevance to a particular context or user group, and clarity of the task or goal. \n",
      "\n",
      "However, based on the information available, it's challenging to grade the statement \"making a friend\" as it doesn't clearly define a specific task or goal. It would be helpful to refine this statement to make it more specific and relevant to a particular context or user group. For example, \"making a friend in a new city\" or \"making a friend in a new job\" would provide more context and specificity.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm=ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True, k=2)\n",
    "\n",
    "result = agent_executor({\"input\": \"can you use rubrics to grade this job to be done statement 'making a friend'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAs an AI language model, I don't have real-time data, so I cannot provide information on the most recent State of the Union address. The State of the Union address is an annual speech delivered by the President of the United States to Congress, typically held in January or February. It provides an opportunity for the President to address the nation, outline their legislative agenda, and discuss the current state of the country. To find information about the most recent State of the Union address, I recommend referring to reliable news sources or official government websites.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"what is recent state of the union?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI apologize for the confusion in my previous response. Since grading rubrics for JTBD are not commonly used or standardized, there may not be specific grading rubrics available. However, I can provide you with some general guidelines that can be considered when evaluating the application of JTBD:\n",
      "\n",
      "1. Problem Identification: Assess the clarity and accuracy of identifying the core problem or job that customers are trying to accomplish. Is the problem well-defined and aligned with customer needs?\n",
      "\n",
      "2. Customer Understanding: Evaluate the depth of understanding of customer motivations, goals, and desired outcomes. How well does the analysis capture the functional, social, and emotional aspects of the job?\n",
      "\n",
      "3. Solution Alignment: Assess the alignment between the identified job and the proposed solutions or offerings. How well do the solutions address the customer's desired outcomes?\n",
      "\n",
      "4. Actionability: Evaluate the practicality and feasibility of applying the JTBD framework to inform product development or marketing strategies. Are the insights actionable and can they be effectively implemented?\n",
      "\n",
      "5. Impact and Results: Assess the effectiveness and impact of applying the JTBD framework. Has it led to improved customer satisfaction, increased sales, or other positive outcomes?\n",
      "\n",
      "6. Communication and Presentation: Evaluate the clarity and effectiveness of communicating the JTBD analysis. Is the information presented in a clear and organized manner?\n",
      "\n",
      "These guidelines can serve as a starting point for creating a grading rubric specific to your needs. It's important to adapt and customize the rubric based on the objectives and requirements of the evaluation or assessment process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"what are grading rubrics for jtbd?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nretriever\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain, RetrievalQA\n\u001b[0;32m----> 3\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# OpenAI(temperature=0), \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# index.vectorstore.as_retriever(\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#     search_kwargs={\"k\": 3}\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#     ),\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# return_source_documents =True,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# return_only_outputs\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain/chains/conversational_retrieval/base.py:383\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    377\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    378\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    379\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    380\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    381\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    382\u001b[0m )\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nretriever\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm=ChatOpenAI(model=\"gpt-4\", temperature=0.1),# OpenAI(temperature=0), \n",
    "  retriever= None\n",
    "  # index.vectorstore.as_retriever(\n",
    "  #     search_kwargs={\"k\": 3}\n",
    "  #     ),\n",
    "  # return_source_documents =True,\n",
    "  # return_only_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import openai\n",
    "# import streamlit as st\n",
    "# import os\n",
    "from langchain.chains import ConversationalRetrievalChain #RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "import random\n",
    "from system_prompt\\\n",
    "  import create_system_prompt, GUIDE_FOR_USERS\n",
    "from constants import RETRIEVER_SEARCH_DEPTH, VECTORSTORE_DIRECTORY_NAME\n",
    "from my_secrets import OPENAI_API_KEY\n",
    "\n",
    "MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "VECTORSTORE_DIRECTORY_NAME\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shayan/Documents/DL folder with python 3.7 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading up vec database..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"source_data/\")\n",
    "# VECTORSTORE_DIRECTORY_NAME = '/vectorstore/'\n",
    "if os.path.exists(VECTORSTORE_DIRECTORY_NAME):\n",
    "  print(\"loading up vec database..\\n\")\n",
    "  vectorstore = Chroma(persist_directory=VECTORSTORE_DIRECTORY_NAME, embedding_function=OpenAIEmbeddings())\n",
    "  index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":VECTORSTORE_DIRECTORY_NAME}).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=index.vectorstore.as_retriever(\n",
    "     search_kwargs={\"k\": 4, \"fetch_k\": 6}, \n",
    "     search_type='mmr'\n",
    "    )\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm= ChatOpenAI(model=MODEL, temperature=0),\n",
    "  retriever=retriever,\n",
    "  verbose=True,\n",
    "  # return_intermediate_steps=True\n",
    ")\n",
    "chain.retriever.invoke(\"grade the use cases\")\n",
    "\n",
    "# grade the use cases\n",
    "\n",
    "# index.vectorstore.('grade the job to be done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 1454, which is longer than the specified 500\n",
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 757, which is longer than the specified 500\n",
      "Created a chunk of size 1114, which is longer than the specified 500\n",
      "Created a chunk of size 695, which is longer than the specified 500\n",
      "Created a chunk of size 580, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 868, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[1;32m     10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[0;32m---> 11\u001b[0m openai_lc_client \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai_collection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvectorstore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# docs = db2.similarity_search(query)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# docs = db3.similarity_search(query)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(docs[0].page_content)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:771\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    770\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:707\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    687\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[1;32m    688\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m         ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid1()) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:126\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/api/client.py:237\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/api/segment.py:216\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, tenant, database)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/api/segment.py:166\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    162\u001b[0m check_index_name(name)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m uuid4()\n\u001b[0;32m--> 166\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[1;32m    177\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mcreate_segments(coll)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/chromadb/db/mixins/sysdb.py:267\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    265\u001b[0m sql, params \u001b[38;5;241m=\u001b[39m get_sql(insert_collection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_format())\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_constraint_error() \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = DirectoryLoader(\"source_data\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs, embeddings, collection_name=\"openai_collection\", persist_directory='vectorstore'\n",
    ")\n",
    "\n",
    "# db2 = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# docs = db2.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "# db3 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "# docs = db3.similarity_search(query)\n",
    "# print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. **Clarity and Specificity**: Use cases should be clear and specific. For example, \"Shopping\" is vague, but \"Shopping online\" is a more defined use case. 2. **Focus on Significance**: Concentrate on the most significant use cases to ensure broad relevance. Avoid overly narrow or niche use cases that have limited utility. 3. **Hierarchy**: Use cases can be organized in a hierarchy. Although sub-use cases are not commonly called such, they exist. For instance, under the main use case \"Deodorizing,\" sub-use cases could include \"Deodorizing refrigerators,\" \"Deodorizing wastebaskets,\" and \"Deodorizing recyclables.\" However, for this course, it\\'s recommended to focus on top-level use cases.', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='There are 4 Criteria for making Use Cases: 1. **Important. ** It should be significant to the actor. 2. **Distinct. ** There are clear, distinctive contextual differences between each use case. 3. **Clearly named. ** It’s specific, unique and unambiguous. (You can put it on a web site.) 4. **Accurate. ** The use case does not include the actor, JTBD, outcome, fertile land, .\\n\\nExamples of valid use cases for the actor “Rider” and the job to be done “**Be driven to a destination**” are as follows:', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_lc_client.similarity_search('grade the use cases')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. **Clarity and Specificity**: Use cases should be clear and specific. For example, \"Shopping\" is vague, but \"Shopping online\" is a more defined use case. 2. **Focus on Significance**: Concentrate on the most significant use cases to ensure broad relevance. Avoid overly narrow or niche use cases that have limited utility. 3. **Hierarchy**: Use cases can be organized in a hierarchy. Although sub-use cases are not commonly called such, they exist. For instance, under the main use case \"Deodorizing,\" sub-use cases could include \"Deodorizing refrigerators,\" \"Deodorizing wastebaskets,\" and \"Deodorizing recyclables.\" However, for this course, it\\'s recommended to focus on top-level use cases.', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='There are 4 Criteria for making Use Cases: 1. **Important. ** It should be significant to the actor. 2. **Distinct. ** There are clear, distinctive contextual differences between each use case. 3. **Clearly named. ** It’s specific, unique and unambiguous. (You can put it on a web site.) 4. **Accurate. ** The use case does not include the actor, JTBD, outcome, fertile land, .\\n\\nExamples of valid use cases for the actor “Rider” and the job to be done “**Be driven to a destination**” are as follows:', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. Exclude concepts like the JTBD or outcomes from the use case to keep it focused. 2. Keep use cases concise and to the point. 3. Aim for accuracy and precision. 4. Avoid introducing other concepts like outcomes within the use case.\\n\\nYou may need to combine multiple contextual factors to name a use case, as in the example: \"Remote meetings on iOS devices.\"\\n\\nAlternative Terms for use cases can be the following, however, they are not used in this course:\\n\\n“Use”\\n\\n“Usage Occasion” (often in the Consumer-Packaged Goods industry),\\n\\n“Operational Scenario” (MSE/Software Engineering)\\n\\n“Application” (often with Manufactured products)\\n\\n“Indication” (often in the Pharma industry)\\n\\n“Solution”', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=openai_lc_client.as_retriever(\n",
    "     search_kwargs={\"k\": 4, \"fetch_k\": 6}, \n",
    "     search_type='mmr'\n",
    "    )\n",
    "retriever.invoke(\"grade the use cases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. Exclude concepts like the JTBD or outcomes from the use case to keep it focused. 2. Keep use cases concise and to the point. 3. Aim for accuracy and precision. 4. Avoid introducing other concepts like outcomes within the use case.\\n\\nYou may need to combine multiple contextual factors to name a use case, as in the example: \"Remote meetings on iOS devices.\"\\n\\nAlternative Terms for use cases can be the following, however, they are not used in this course:\\n\\n“Use”\\n\\n“Usage Occasion” (often in the Consumer-Packaged Goods industry),\\n\\n“Operational Scenario” (MSE/Software Engineering)\\n\\n“Application” (often with Manufactured products)\\n\\n“Indication” (often in the Pharma industry)\\n\\n“Solution”', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='A use case serves as a target circumstance for your product idea\\'s application. Each use case can lead to different product features, products, pricing, designs, packaging, marketing messages, and more.\\n\\nHere are some key points regarding use cases:\\n\\n1. **Clarity and Specificity**: Use cases should be clear and specific. For example, \"Shopping\" is vague, but \"Shopping online\" is a more defined use case. 2. **Focus on Significance**: Concentrate on the most significant use cases to ensure broad relevance. Avoid overly narrow or niche use cases that have limited utility. 3. **Hierarchy**: Use cases can be organized in a hierarchy. Although sub-use cases are not commonly called such, they exist. For instance, under the main use case \"Deodorizing,\" sub-use cases could include \"Deodorizing refrigerators,\" \"Deodorizing wastebaskets,\" and \"Deodorizing recyclables.\" However, for this course, it\\'s recommended to focus on top-level use cases.', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='# 🚀Use cases\\n\\nUse cases\\n\\nDefinition: Use Case: A specific circumstance or set of circumstances in which the actor (customer) does the Job To Be Done. Use cases are specific to each job to be done and each actor.\\n\\nThere are 4 Criteria for making Use Cases: 1. **Important. ** It should be significant to the actor. 2. **Distinct. ** There are clear, distinctive contextual differences between each use case. 3. **Clearly named. ** It’s specific, unique and unambiguous. (You can put it on a web site.) 4. **Accurate. ** The use case does not include the actor, JTBD, outcome, fertile land, .\\n\\nExamples of valid use cases for the actor “Rider” and the job to be done “**Be driven to a destination**” are as follows:\\n\\n1. “During extreme weather” 2. “During peak hours”, “During normal hours” 3. “while riding with others”, “while riding alone”', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"grade the use cases\"\n",
    "openai_lc_client.similarity_search(query, search_kwargs={\"k\": 1, \"fetch_k\": 6}, \n",
    "     search_type='mmr' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm= ChatOpenAI(model=MODEL, temperature=0),\n",
    "  retriever=index.vectorstore.as_retriever(\n",
    "     search_kwargs={\"k\": RETRIEVER_SEARCH_DEPTH}, \n",
    "     search_type='mmr'\n",
    "    ),\n",
    "  verbose=True,\n",
    "  # return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    # Mission\\n    You are a helpful assistant for Product Management Essentials, a course taught by Jim Berardone at Carnegie Mellon University. The course is targeted towards software engineers.\\n\\n    Teaching assistants will need help grading students work.\\n    When asked to grade, use relevant rubrics to give a score at the start of your answer followed by a short explanation.\\n    Students will need help with coming up with ideas, or questions about the concepts.\\n    --\\n    \\n    # Context\\n    The following is distilled list of all the topics in the course packed as succinct statements, assertions, associations, concepts, analogies, and metaphors:\\n    - Product Management Essentials course by Jim Berardone at Carnegie Mellon University.\\n- Target audience: software engineers learning product management fundamentals.\\n- Concepts: specific, non-loose definitions compared to industry/internet usage.\\n\\nFertile Land:\\n- Definition: problem-rich areas for product/solution development.\\n- Purpose: identify familiar problem spaces for students.\\n- Scope: domain-agnostic, multiple problem spaces.\\n- Current state (\"As Is\") vs. future state (\"To Be\") transition through product implementation.\\n- Examples: public transportation, space exploration, online gaming, sporting events, farming, airline travel.\\n\\nProblem Space:\\n- Components: Actor, Job To Be Done (JTBD), Use Case, Actual Outcomes, Desired Outcomes, Problem, Causes, Problem Size, Opportunity Size.\\n\\nActor:\\n- Definition: customer class interacting with future product.\\n- Importance: defines problem space, not specific problem.\\n- Examples: Rider in public transportation, Fan in sporting events, Ticketholder in music concerts, Cardholder in payment processing.\\n- Balance: not too general/specific, avoid geographic specificity.\\n- Alternative terms: Customer, Customer Persona, User Persona, Persona, Customer Archetype, User Archetype.\\n\\nJob To Be Done (JTBD):\\n- Definition: fundamental task actor seeks to accomplish.\\n- Rules: Functional, Singular, Accurate, Appropriate.\\n- Analogy: product as employee hired for a job.\\n- Examples: \"Plant crops\" for Farmer, \"Feed animals\" for Animal rearer.\\n\\nUse Cases:\\n- Definition: specific circumstances for actor\\'s JTBD.\\n- Criteria: Important, Distinct, Clearly named, Accurate.\\n- Examples: \"During extreme weather,\" \"During peak hours,\" \"while riding with others\" for Rider\\'s JTBD \"Be driven to a destination.\"\\n- Contextual prompts: Locations, Environments, Timings, Tools/Objects.\\n\\nCauses:\\n- Types: Root Causes, Contributing Causes.\\n- Techniques: Judgment, 5 Whys, Regression Analysis, Randomized Controlled Trials, Propensity Score Analysis, Causal Inference, Ishikawa Fishbone Diagram, Fault Tree Analysis, Failure Mode Effects Analysis, Pareto Analysis.\\n\\nOutcomes:\\n- Definition: bridging gap between actual and desired outcomes through product use.\\n- Components: Metric, Quantified Measurement, Numerical Value.\\n- Metrics: Duration, Effort, Speed, Quantity, Errors, Risk, Decision quality, etc.\\n- Process: customer interviews, surveys, focus groups, social listening, data analysis.\\n\\nProblem:\\n- Definition: gap between actual and desired outcomes.\\n- Complexity: multiple actors with different outcomes and jobs.\\n- Terms: Results, Consequences, Key Results, Achievements, Experiences.\\n\\nProblem Communication:\\n- Problem Category: concise label for problem type.\\n- Problem Statement: clear, factual description including essential elements.\\n- Problem Narratives: in-depth stories with real examples and personas.\\n\\nProblem Magnitude:\\n- Definition: size and extent of problem.\\n- Components: Amount, Severity, Frequency, Probability, Growth Rate.\\n- Annualizing: standardizing problem size comparison.\\n- Opportunity Size: number of individuals experiencing the problem.\\n\\nPersonas and Roles:\\n- Criteria: Specific name, Individual stakeholder, Accurate, Distinct.\\n- Essential roles: User, Decider, Buyer, Influencer, Gatekeeper, Initiator.\\n    --\\n    # Rules when generating answers\\n    1. Always follow instructions. Only follow instructions.\\n    2. Never argue with the user unless they solicit feedback.\\n    3. Ask thoughtful questions only when appropriate.\\n    4. Never explain things unless asked to do so. Never give long summaries of your own responses.\\n    5. When asked question that has nothing to do product management or the course, you are to politely reject the question.\\n    6. Never explain that you are a chatbot. The user knows this. Just follow the intention of the user.\\n    7. When given additional context, if there is conflicting information pick only the most relevant and specific information.\\n    8. When asked to grade work, always give the final marks on the top.\\n    --\\n    \\n    --\\n    # StepsTake a deep breath and think through it step by step.\\n    ## Step 1: understand the user\\'s query\\n\\n    ## Step 2: analyse the known information and the context\\n\\n    ## Step 3: Call functions to fill in missing/additional information\\n\\n    ## Step 4: give an answer\\n      give a concise explanation, the shorter the better.\\n\\n\\n    '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from system_prompt import create_system_prompt\n",
    "create_system_prompt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
