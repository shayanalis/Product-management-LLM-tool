{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "# from my_secrets import OPENAI_API_KEY\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 565, which is longer than the specified 500\n",
      "Created a chunk of size 695, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 1454, which is longer than the specified 500\n",
      "Created a chunk of size 580, which is longer than the specified 500\n",
      "Created a chunk of size 533, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 914, which is longer than the specified 500\n",
      "Created a chunk of size 701, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n",
      "Created a chunk of size 703, which is longer than the specified 500\n",
      "Created a chunk of size 571, which is longer than the specified 500\n",
      "Created a chunk of size 537, which is longer than the specified 500\n",
      "Created a chunk of size 1114, which is longer than the specified 500\n",
      "Created a chunk of size 757, which is longer than the specified 500\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'OpenAI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      5\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n\u001b[0;32m----> 6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(texts, embeddings)\n\u001b[1;32m      9\u001b[0m retriever \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_community/embeddings/openai.py:350\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    339\u001b[0m client_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morganization\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai_organization\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    348\u001b[0m }\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 350\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params)\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    352\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_client\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params\n\u001b[1;32m    354\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'OpenAI'"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader('./source_data')\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='## Grading rubric for the outcome statement: 3 marks if All outcomes meet criteria 2 marks if Most outcomes meet these criteria: 1 mark if Few outcomes meet these criteria: 0 marks if No outcomes meet these criteria:', metadata={'source': 'data/Rubric_outcomes.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. Clearly named. The name is specific, unique and unambiguous. 2. Accurate. The name does not include a persona, benefit, outcome or JTBD 3. Current state. Represents a scenario that exists currently without your product. 4. Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'data/Rubric_use_cases.txt'}),\n",
       " Document(page_content='Grading Rubric for Problem Space:\\n\\n2 pts: Full Marks for 2 fully-completed problem space candidates were provided. 1 pts: Partial Credit: Only 1 Problem Space candidate was fully-completed. 0 pts: Unacceptable: None of the 2 problem space candidates were fully-completed.', metadata={'source': 'data/Rubricassignment1.txt'}),\n",
       " Document(page_content='## Grading rubric for the Use cases: 3 marks if All use cases satisfy these criteria: 2 marks if Most use cases satisfy these criteria: 1 marks if Few use cases satisfy these criteria: 0 No use cases satisfy these criteria:', metadata={'source': 'data/Rubric_use_cases.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retriever.get_relevant_documents(\n",
    "    \"rubrics?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_for_job_to_be_done_definition\",\n",
    "    \"searches for definition and rubrics for job to be done statement\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='search for job to be done', description='searches for definition and rubrics for job to be done statement', args_schema=<class 'langchain.tools.retriever.RetrieverInput'>, func=<bound method BaseRetriever.get_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17cca0400>)>, coroutine=<bound method BaseRetriever.aget_relevant_documents of VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x17cca0400>)>)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_for_job_to_be_done_definition` with `{'query': 'making a friend'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(page_content='for students \"jobs to be done\" include “pursuing a master\\'s degree”, “find housing”, “commuting”, “dining”, “maintaining living space”, “securing employment”, and “traveling”. \"Getting an education\" is one of the central primary jobs. Other primary tasks might include \"attending classes”, “finding part-time work”, “securing internships”, and “dedicating time to study”.\\n\\nUnderstanding all these different jobs could lead you to creating many different products which all businesses eventually need.', metadata={'source': 'data/JTBD.txt'}), Document(page_content='here are a few avenues to explore when gathering data and insights:', metadata={'source': 'data/problem_magnitude.txt'}), Document(page_content='A Fertile Land of Online shopping could have an actor Shopper and merchant\\n\\nThe actor can be as general as \"Teacher,\" encompassing all teachers worldwide, or more specific, such as \"College Teachers,\" narrowing it down to educators at the college level globally. For further precision, you can specify an actor like \"College Teachers at 300 South Craig St, Room 265,\" targeting teachers in a particular room.', metadata={'source': 'data/actor.txt'}), Document(page_content=\"In some instances, it might be challenging to find a precise number. Take, for example, a scenario like the one faced by a student in the class who was dealing with fitness enthusiasts. This can be a complex group to quantify since individuals don't typically wear a label proclaiming their fitness enthusiasm. Thus, determining the opportunity size may require creative strategies, but at its core, it's simply about estimating the number of people who match the characteristics of your actor.\", metadata={'source': 'data/problem_magnitude.txt'})]\u001b[0m\u001b[32;1m\u001b[1;3mThe job to be done statement \"making a friend\" is quite broad and lacks specificity. Jobs to be done statements are typically more specific and tied to a particular task or goal. For example, in a student context, jobs to be done might include \"pursuing a master's degree\", \"finding housing\", or \"securing employment\". \n",
      "\n",
      "To grade this statement, we would need to establish clear rubrics or criteria. These might include specificity, relevance to a particular context or user group, and clarity of the task or goal. \n",
      "\n",
      "However, based on the information available, it's challenging to grade the statement \"making a friend\" as it doesn't clearly define a specific task or goal. It would be helpful to refine this statement to make it more specific and relevant to a particular context or user group. For example, \"making a friend in a new city\" or \"making a friend in a new job\" would provide more context and specificity.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm=ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "agent_executor = create_conversational_retrieval_agent(llm, tools, verbose=True, k=2)\n",
    "\n",
    "result = agent_executor({\"input\": \"can you use rubrics to grade this job to be done statement 'making a friend'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAs an AI language model, I don't have real-time data, so I cannot provide information on the most recent State of the Union address. The State of the Union address is an annual speech delivered by the President of the United States to Congress, typically held in January or February. It provides an opportunity for the President to address the nation, outline their legislative agenda, and discuss the current state of the country. To find information about the most recent State of the Union address, I recommend referring to reliable news sources or official government websites.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"what is recent state of the union?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI apologize for the confusion in my previous response. Since grading rubrics for JTBD are not commonly used or standardized, there may not be specific grading rubrics available. However, I can provide you with some general guidelines that can be considered when evaluating the application of JTBD:\n",
      "\n",
      "1. Problem Identification: Assess the clarity and accuracy of identifying the core problem or job that customers are trying to accomplish. Is the problem well-defined and aligned with customer needs?\n",
      "\n",
      "2. Customer Understanding: Evaluate the depth of understanding of customer motivations, goals, and desired outcomes. How well does the analysis capture the functional, social, and emotional aspects of the job?\n",
      "\n",
      "3. Solution Alignment: Assess the alignment between the identified job and the proposed solutions or offerings. How well do the solutions address the customer's desired outcomes?\n",
      "\n",
      "4. Actionability: Evaluate the practicality and feasibility of applying the JTBD framework to inform product development or marketing strategies. Are the insights actionable and can they be effectively implemented?\n",
      "\n",
      "5. Impact and Results: Assess the effectiveness and impact of applying the JTBD framework. Has it led to improved customer satisfaction, increased sales, or other positive outcomes?\n",
      "\n",
      "6. Communication and Presentation: Evaluate the clarity and effectiveness of communicating the JTBD analysis. Is the information presented in a clear and organized manner?\n",
      "\n",
      "These guidelines can serve as a starting point for creating a grading rubric specific to your needs. It's important to adapt and customize the rubric based on the objectives and requirements of the evaluation or assessment process.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor(\n",
    "    {\n",
    "        \"input\": \"what are grading rubrics for jtbd?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nretriever\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationalRetrievalChain, RetrievalQA\n\u001b[0;32m----> 3\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# OpenAI(temperature=0), \u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# index.vectorstore.as_retriever(\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#     search_kwargs={\"k\": 3}\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#     ),\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# return_source_documents =True,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# return_only_outputs\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain/chains/conversational_retrieval/base.py:383\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    377\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    378\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    379\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    380\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    381\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    382\u001b[0m )\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/langchain_core/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.6/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nretriever\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm=ChatOpenAI(model=\"gpt-4\", temperature=0.1),# OpenAI(temperature=0), \n",
    "  retriever= None\n",
    "  # index.vectorstore.as_retriever(\n",
    "  #     search_kwargs={\"k\": 3}\n",
    "  #     ),\n",
    "  # return_source_documents =True,\n",
    "  # return_only_outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import openai\n",
    "# import streamlit as st\n",
    "# import os\n",
    "from langchain.chains import ConversationalRetrievalChain #RetrievalQA\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "import random\n",
    "from system_prompt\\\n",
    "  import create_system_prompt, GUIDE_FOR_USERS\n",
    "from constants import RETRIEVER_SEARCH_DEPTH, VECTORSTORE_DIRECTORY_NAME\n",
    "from my_secrets import OPENAI_API_KEY\n",
    "\n",
    "MODEL = \"gpt-4-1106-preview\"\n",
    "\n",
    "VECTORSTORE_DIRECTORY_NAME\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shayan/Documents/DL folder with python 3.7 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading up vec database..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"source_data/\")\n",
    "# VECTORSTORE_DIRECTORY_NAME = '/vectorstore/'\n",
    "if os.path.exists(VECTORSTORE_DIRECTORY_NAME):\n",
    "  print(\"loading up vec database..\\n\")\n",
    "  vectorstore = Chroma(persist_directory=VECTORSTORE_DIRECTORY_NAME, embedding_function=OpenAIEmbeddings())\n",
    "  index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":VECTORSTORE_DIRECTORY_NAME}).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='Criteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=index.vectorstore.as_retriever(\n",
    "     search_kwargs={\"k\": 4, \"fetch_k\": 6}, \n",
    "     search_type='mmr'\n",
    "    )\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm= ChatOpenAI(model=MODEL, temperature=0),\n",
    "  retriever=retriever,\n",
    "  verbose=True,\n",
    "  # return_intermediate_steps=True\n",
    ")\n",
    "chain.retriever.invoke(\"grade the use cases\")\n",
    "\n",
    "# grade the use cases\n",
    "\n",
    "# index.vectorstore.('grade the job to be done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1454, which is longer than the specified 1000\n",
      "Created a chunk of size 1114, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "loader = DirectoryLoader(\"source_data\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "new_client = chromadb.EphemeralClient()\n",
    "openai_lc_client = Chroma.from_documents(\n",
    "    docs, embeddings, client=new_client, collection_name=\"openai_collection\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. Exclude concepts like the JTBD or outcomes from the use case to keep it focused. 2. Keep use cases concise and to the point. 3. Aim for accuracy and precision. 4. Avoid introducing other concepts like outcomes within the use case.\\n\\nYou may need to combine multiple contextual factors to name a use case, as in the example: \"Remote meetings on iOS devices.\"\\n\\nAlternative Terms for use cases can be the following, however, they are not used in this course:\\n\\n“Use”\\n\\n“Usage Occasion” (often in the Consumer-Packaged Goods industry),\\n\\n“Operational Scenario” (MSE/Software Engineering)\\n\\n“Application” (often with Manufactured products)\\n\\n“Indication” (often in the Pharma industry)\\n\\n“Solution”', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='A use case serves as a target circumstance for your product idea\\'s application. Each use case can lead to different product features, products, pricing, designs, packaging, marketing messages, and more.\\n\\nHere are some key points regarding use cases:\\n\\n1. **Clarity and Specificity**: Use cases should be clear and specific. For example, \"Shopping\" is vague, but \"Shopping online\" is a more defined use case. 2. **Focus on Significance**: Concentrate on the most significant use cases to ensure broad relevance. Avoid overly narrow or niche use cases that have limited utility. 3. **Hierarchy**: Use cases can be organized in a hierarchy. Although sub-use cases are not commonly called such, they exist. For instance, under the main use case \"Deodorizing,\" sub-use cases could include \"Deodorizing refrigerators,\" \"Deodorizing wastebaskets,\" and \"Deodorizing recyclables.\" However, for this course, it\\'s recommended to focus on top-level use cases.', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='# 🚀Use cases\\n\\nUse cases\\n\\nDefinition: Use Case: A specific circumstance or set of circumstances in which the actor (customer) does the Job To Be Done. Use cases are specific to each job to be done and each actor.\\n\\nThere are 4 Criteria for making Use Cases: 1. **Important. ** It should be significant to the actor. 2. **Distinct. ** There are clear, distinctive contextual differences between each use case. 3. **Clearly named. ** It’s specific, unique and unambiguous. (You can put it on a web site.) 4. **Accurate. ** The use case does not include the actor, JTBD, outcome, fertile land, .\\n\\nExamples of valid use cases for the actor “Rider” and the job to be done “**Be driven to a destination**” are as follows:\\n\\n1. “During extreme weather” 2. “During peak hours”, “During normal hours” 3. “while riding with others”, “while riding alone”', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_lc_client.similarity_search('grade the use cases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='“Solution”\\n\\nCriteria for Grading rubric for the Use cases: 1. (1 mark) Clearly named. The name is specific, unique and unambiguous. 2. (1 mark) Accurate. The name does not include a persona, benefit, outcome or JTBD 3. (1 mark) Current state. Represents a scenario that exists currently without your product. 4. (1 mark) Distinct. There are clear, distinctive differences between each use case', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='1. Exclude concepts like the JTBD or outcomes from the use case to keep it focused. 2. Keep use cases concise and to the point. 3. Aim for accuracy and precision. 4. Avoid introducing other concepts like outcomes within the use case.\\n\\nYou may need to combine multiple contextual factors to name a use case, as in the example: \"Remote meetings on iOS devices.\"\\n\\nAlternative Terms for use cases can be the following, however, they are not used in this course:\\n\\n“Use”\\n\\n“Usage Occasion” (often in the Consumer-Packaged Goods industry),\\n\\n“Operational Scenario” (MSE/Software Engineering)\\n\\n“Application” (often with Manufactured products)\\n\\n“Indication” (often in the Pharma industry)\\n\\n“Solution”', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='A use case serves as a target circumstance for your product idea\\'s application. Each use case can lead to different product features, products, pricing, designs, packaging, marketing messages, and more.\\n\\nHere are some key points regarding use cases:\\n\\n1. **Clarity and Specificity**: Use cases should be clear and specific. For example, \"Shopping\" is vague, but \"Shopping online\" is a more defined use case. 2. **Focus on Significance**: Concentrate on the most significant use cases to ensure broad relevance. Avoid overly narrow or niche use cases that have limited utility. 3. **Hierarchy**: Use cases can be organized in a hierarchy. Although sub-use cases are not commonly called such, they exist. For instance, under the main use case \"Deodorizing,\" sub-use cases could include \"Deodorizing refrigerators,\" \"Deodorizing wastebaskets,\" and \"Deodorizing recyclables.\" However, for this course, it\\'s recommended to focus on top-level use cases.', metadata={'source': 'source_data/3_usecases.txt'}),\n",
       " Document(page_content='# 🚀Use cases\\n\\nUse cases\\n\\nDefinition: Use Case: A specific circumstance or set of circumstances in which the actor (customer) does the Job To Be Done. Use cases are specific to each job to be done and each actor.\\n\\nThere are 4 Criteria for making Use Cases: 1. **Important. ** It should be significant to the actor. 2. **Distinct. ** There are clear, distinctive contextual differences between each use case. 3. **Clearly named. ** It’s specific, unique and unambiguous. (You can put it on a web site.) 4. **Accurate. ** The use case does not include the actor, JTBD, outcome, fertile land, .\\n\\nExamples of valid use cases for the actor “Rider” and the job to be done “**Be driven to a destination**” are as follows:\\n\\n1. “During extreme weather” 2. “During peak hours”, “During normal hours” 3. “while riding with others”, “while riding alone”', metadata={'source': 'source_data/3_usecases.txt'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"grade the use cases\"\n",
    "openai_lc_client.similarity_search(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
